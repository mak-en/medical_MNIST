{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_file.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOlLSFRtXHJ1XcGm4r9X99f"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZAhpo9y3FvL"
      },
      "source": [
        "# Download data\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d 'andrewmvd/medical-mnist'\n",
        "!unzip -q medical-mnist.zip -d data\n",
        "!rm medical-mnist.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSOXj_vP9Vj_"
      },
      "source": [
        "# Installations\n",
        "!pip install pytorch-lightning\n",
        "!pip install wandb\n",
        "!pip install optuna"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6ynB7HTXdR"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch\n",
        "import torchmetrics\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback\n",
        "import joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P-c3p3rKjPZ"
      },
      "source": [
        "# Data\n",
        "class MedicalDataMNIST(pl.LightningDataModule):\n",
        "    def __init__(self, model=\"EfficientNetb0\", batch_size=64, num_workers=1):\n",
        "      super().__init__()\n",
        "      \n",
        "      self.num_workers = num_workers\n",
        "      self.batch_size = batch_size\n",
        "      self.labels_map = {0 : \"AbdomenCT\",\n",
        "                         1 : \"BreastMRI\",\n",
        "                         2 : \"CXR\",\n",
        "                         3 : \"ChestCT\",\n",
        "                         4 : \"Hand\",\n",
        "                         5 : \"HeadCT\"}\n",
        "      self.model = model\n",
        "      self.train_transform = transforms.Compose(\n",
        "          [transforms.ColorJitter(hue=.20, saturation=.20),\n",
        "           transforms.RandomHorizontalFlip(),\n",
        "           transforms.RandomVerticalFlip(),\n",
        "           transforms.RandomRotation(10),\n",
        "           transforms.ToTensor(),\n",
        "           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])]\n",
        "      )\n",
        "      self.train_inception_transform = transforms.Compose(\n",
        "          [transforms.Resize(299),\n",
        "           transforms.ColorJitter(hue=.20, saturation=.20),\n",
        "           transforms.RandomHorizontalFlip(),\n",
        "           transforms.RandomVerticalFlip(),\n",
        "           transforms.RandomRotation(10),\n",
        "           transforms.ToTensor(),\n",
        "           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])]\n",
        "      )      \n",
        "      self.val_test_transform = transforms.Compose(\n",
        "          [transforms.ToTensor(),\n",
        "           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])]\n",
        "      )\n",
        "      self.val_test_inception_transform = transforms.Compose(\n",
        "          [transforms.Resize(299),\n",
        "           transforms.ToTensor(),\n",
        "           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])]\n",
        "      )\n",
        "\n",
        "    def prepare_data(self):\n",
        "      pass\n",
        "      # called only on 1 GPU\n",
        "      # ONLY DOWNLOAD!!!\n",
        "      \n",
        "    def setup(self, stage=None):\n",
        "\n",
        "      self.dataset = ImageFolder(\"./data/\")\n",
        "      train_size = int(0.7 * len(self.dataset)) # take 70% for training\n",
        "      val_size = int(0.2 * len(self.dataset)) # take 20% for validation\n",
        "      test_size = len(self.dataset) - (train_size + val_size) # take 10% for test\n",
        "      \n",
        "      self.train_set, self.val_set, self.test_set = \\\n",
        "      torch.utils.data.random_split(self.dataset, \n",
        "                                    [train_size, val_size, test_size])\n",
        "      if self.model == \"InceptionV3\":\n",
        "        self.train_set.dataset.transform = self.train_inception_transform\n",
        "        self.val_set.dataset.transform = self.val_test_inception_transform\n",
        "        self.test_set.dataset.transform = self.val_test_inception_transform\n",
        "      else:\n",
        "        self.train_set.dataset.transform = self.train_transform\n",
        "        self.val_set.dataset.transform = self.val_test_transform\n",
        "        self.test_set.dataset.transform = self.val_test_transform\n",
        "\n",
        "    def train_dataloader(self):\n",
        "      return DataLoader(self.train_set, \n",
        "                        batch_size=self.batch_size, \n",
        "                        shuffle=True, \n",
        "                        num_workers=self.num_workers) \n",
        "\n",
        "    def val_dataloader(self):\n",
        "      return DataLoader(self.val_set, \n",
        "                        batch_size=self.batch_size, \n",
        "                        num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "      print(\"TEST DATALOADER\")\n",
        "      return DataLoader(self.test_set, \n",
        "                        batch_size=self.batch_size, \n",
        "                        num_workers=self.num_workers)\n",
        "\n",
        "    def visualize_dataset(self):\n",
        "      # Visualizes dataset\n",
        "      figure = plt.figure(figsize=(8, 8))\n",
        "      cols, rows = 3, 3\n",
        "      for i in range(1, cols * rows + 1):\n",
        "          sample_idx = torch.randint(len(self.train_set), size=(1,)).item()\n",
        "          norm_img, label = self.train_set[sample_idx]\n",
        "          mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "          std = torch.tensor([0.229, 0.224, 0.225])\n",
        "          img = norm_img * std[:, None, None] + mean[:, None, None] \n",
        "          figure.add_subplot(rows, cols, i)\n",
        "          plt.title(self.labels_map[label])\n",
        "          plt.axis(\"off\")\n",
        "          plt.imshow(img.permute(1, 2, 0))\n",
        "      plt.show()\n",
        "\n",
        "    def visualize_dataloader(self):\n",
        "      # Display image and label\n",
        "      train_dataloader = self.train_dataloader()\n",
        "      train_features, train_labels = next(iter(train_dataloader))\n",
        "      print(f\"Feature batch shape: {train_features.size()}\")\n",
        "      print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "      norm_img = train_features[0]\n",
        "      mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "      std = torch.tensor([0.229, 0.224, 0.225])\n",
        "      img = norm_img * std[:, None, None] + mean[:, None, None]\n",
        "      label = train_labels[0]\n",
        "      plt.imshow(img.permute(1, 2, 0))\n",
        "      plt.show()\n",
        "      print(f\"Label: {self.labels_map[label.item()]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZsfSpK17vhE"
      },
      "source": [
        "dm = MedicalDataMNIST()\n",
        "dm.setup()\n",
        "dm.visualize_dataset()\n",
        "dm.visualize_dataloader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sihoHTSWNoPA"
      },
      "source": [
        "# Model\n",
        "class MedicalMNIST(LightningModule):\n",
        "    def __init__(self, model=\"EfficientNetb0\", optimizer=\"Adam\", lr=1e-4,\n",
        "                 betas=(0.9, 0.999), eps=1e-08, weight_decay=0, momentum=0,\n",
        "                 alpha=0.99, lambd=1e-4, asgd_alpha=0.75, dropout=0.2): \n",
        "        super().__init__()\n",
        "        self.save_hyperparameters() # saves hyperparameters in ModelCheckpoint\n",
        "        # Hyperparameters\n",
        "        # Model\n",
        "        if model == \"EfficientNetb0\":\n",
        "          # Fine tuning EfficientNetb0\n",
        "          self.name = \"EfficientNetb0\"\n",
        "          self.model = models.efficientnet_b0(pretrained=True)\n",
        "          self.model.classifier = torch.nn.Sequential(\n",
        "              torch.nn.Dropout(p=dropout, inplace=False),\n",
        "              torch.nn.Linear(in_features=self.model.classifier[1].in_features,\n",
        "                              out_features=6)\n",
        "          )\n",
        "\n",
        "        elif model == \"VGG16\":\n",
        "          # Fine tuning VGG16\n",
        "          self.name = \"VGG16\"\n",
        "          self.model = models.vgg16(pretrained=True)\n",
        "          self.model.classifier[-1] = torch.nn.Linear(in_features=4096, \n",
        "                                                out_features=6)\n",
        "        \n",
        "        elif model == \"InceptionV3\":\n",
        "          # Fine tuning InceptionV3\n",
        "          self.name = \"InceptionV3\"\n",
        "          self.model = models.inception_v3(pretrained=True)\n",
        "          # Handle the auxilary net\n",
        "          in_features = self.model.AuxLogits.fc.in_features\n",
        "          self.model.AuxLogits.fc = torch.nn.Linear(in_features=in_features,\n",
        "                                              out_features=6)\n",
        "          # Handle the primary net\n",
        "          in_features = self.model.fc.in_features\n",
        "          self.model.fc = torch.nn.Linear(in_features=in_features,\n",
        "                                    out_features=6)\n",
        "       \n",
        "        else:\n",
        "          # Fine tuning ResNet18\n",
        "          self.name = \"ResNet18\"\n",
        "          self.model = models.resnet18(pretrained=True)\n",
        "          self.model.fc = torch.nn.Linear(in_features=self.model.fc.in_features,\n",
        "                                    out_features=6)\n",
        "\n",
        "        # Optimizer\n",
        "        if optimizer == \"Adam\":\n",
        "          self.optimizer = torch.optim.Adam(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              betas=betas,\n",
        "              eps=eps,\n",
        "              weight_decay=weight_decay\n",
        "          )\n",
        "\n",
        "        elif optimizer == \"SGD\":\n",
        "          self.optimizer = torch.optim.SGD(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              momentum=momentum,\n",
        "              weight_decay=weight_decay\n",
        "          )\n",
        "          \n",
        "        elif optimizer == \"RMSprop\":\n",
        "          self.optimizer = torch.optim.RMSprop(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              alpha=alpha,\n",
        "              eps=eps,\n",
        "              weight_decay=weight_decay,\n",
        "              momentum=momentum\n",
        "          )\n",
        "\n",
        "        else:\n",
        "          # ASGD\n",
        "          self.optimizer = torch.optim.ASGD(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              lambd=lambd,\n",
        "              alpha=asgd_alpha,\n",
        "              weight_decay=weight_decay,\n",
        "          )\n",
        "        \n",
        "        # Metrics\n",
        "        self.train_acc = torchmetrics.Accuracy()\n",
        "        self.val_acc = torchmetrics.Accuracy()\n",
        "        self.train_f1 = torchmetrics.F1(num_classes=6)\n",
        "        self.val_f1 = torchmetrics.F1(num_classes=6)\n",
        "        self.test_f1 = torchmetrics.F1(num_classes=6)\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.model(x)\n",
        "      return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      if self.name == \"InceptionV3\":\n",
        "        output = self.model(x)\n",
        "        loss = torch.nn.functional.cross_entropy(output.logits, y)\n",
        "      else:\n",
        "        logits = self.model(x)\n",
        "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
        "\n",
        "      self.log(\"loss/train\", loss, on_step=True, on_epoch=True)\n",
        "    \n",
        "      return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      y_hat = self.model(x)\n",
        "      val_loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
        "      \n",
        "      self.log(\"loss/val\", val_loss)\n",
        "      self.log(\"accuaracy/val\", self.val_acc(y_hat, y), prog_bar=True,\n",
        "                logger=True)\n",
        "      self.log(\"f1/val\", self.val_f1(y_hat, y), prog_bar=True,\n",
        "                logger=True)\n",
        "      self.log(\"hp_metric\", self.val_f1(y_hat, y), prog_bar=False,\n",
        "                logger=True)\n",
        "      \n",
        "      return val_loss\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "      x, y = batch\n",
        "      logits = self(x)\n",
        "      loss = torch.nn.functional.cross_entropy(logits, y)\n",
        "      self.log(\"f1/test\", self.test_f1(logits, y))\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      return self.optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Best_f1\n",
        "# class BestF1():\n",
        "#   \"\"\"\n",
        "#   Wrapper for f1 metric.\n",
        "#   Implements set() and get()\n",
        "#   \"\"\"\n",
        "#   def __init__(self, f1=0):\n",
        "    \n",
        "#     self.best_f1 = f1\n",
        "\n",
        "#   def get_f1():\n",
        "#     return self.best_f1\n",
        "\n",
        "#   def set_f1(f1):\n",
        "#     self.best_f1 = f1"
      ],
      "metadata": {
        "id": "kjwYfZi7_V3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Custom callbaks\n",
        "# class SaveModelCallback(pl.Callback):\n",
        "#   \"\"\"\n",
        "#   Save best model\n",
        "#   \"\"\"\n",
        "#   def __init__(self, model_name=None, trial=None):\n",
        "#       super().__init__()\n",
        "      \n",
        "#       self.model_name = model_name\n",
        "#       self.trial = trial\n",
        "\n",
        "#   def on_validation_epoch_end(self, trainer, _):\n",
        "#     current_f1 = trainer.callback_metrics[\"f1/val\"].item()\n",
        "#     if self.trial.number == 0 and trainer.current_epoch == 0:\n",
        "#       # global BEST_F1 \n",
        "#       metric = BestF1(current_f1)\n",
        "#       dir = \"./best_model/\"\n",
        "#       files = glob.glob(dir + '*')\n",
        "#       for f in files:\n",
        "#         os.remove(f)\n",
        "#       file_name = f\"trial-{self.trial.number}_{self.model_name}_f1={current_f1:0.2f}.ckpt\"\n",
        "#       ckpt_path = os.path.join(dir, file_name)\n",
        "#       trainer.save_checkpoint(ckpt_path)\n",
        "#     elif current_f1 > metric.best_f1:\n",
        "#       # global BEST_F1 \n",
        "#       BEST_F1 = current_f1\n",
        "#       dir = \"./best_model/\"\n",
        "#       files = glob.glob(dir + '*')\n",
        "#       for f in files:\n",
        "#         os.remove(f)\n",
        "#       file_name = f\"trial-{self.trial.number}_{self.model_name}_f1={current_f1:0.2f}.ckpt\"\n",
        "#       ckpt_path = os.path.join(dir, file_name)\n",
        "#       trainer.save_checkpoint(ckpt_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "kRUtWULV1nn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "  # OPTUNA objective function\n",
        "\n",
        "  # Hyperparameters\n",
        "  model_name = trial.suggest_categorical(\n",
        "      \"model_name\", [\"EfficientNetb0\", \"VGG16\", \"InceptionV3\", \"ResNet18\"]\n",
        "  )\n",
        "  optimizer_name = trial.suggest_categorical(\n",
        "      \"optimaizer_name\", [\"Adam\", \"SGD\", \"RMSprop\", \"ASGD\"]\n",
        "  )\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
        "  betas = (trial.suggest_uniform(\"beta_1\", 0.8, 0.95),\n",
        "            trial.suggest_uniform(\"beta_2\", 0.995, 0.9999))\n",
        "  eps = trial.suggest_loguniform(\"eps\", 1e-09, 1e-07)\n",
        "  weight_decay = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
        "  momentum = trial.suggest_float(\"momentum\", 1e-5, 1e-1, log=True)\n",
        "  alpha = trial.suggest_uniform(\"alpha\", 0.9, 1)\n",
        "  lambd = trial.suggest_float(\"lambd\", 1e-5, 1e-2, log=True)\n",
        "  asgd_alpha = trial.suggest_uniform(\"asgd_alpha\", 0.7, 0.8)\n",
        "  dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "\n",
        "  # Model and data\n",
        "  model = MedicalMNIST(\n",
        "      model=model_name,\n",
        "      optimizer=optimizer_name,\n",
        "      lr=learning_rate,\n",
        "      betas=betas,\n",
        "      eps=eps,\n",
        "      weight_decay=weight_decay,\n",
        "      momentum=momentum,\n",
        "      alpha=alpha,\n",
        "      lambd=lambd,\n",
        "      asgd_alpha=asgd_alpha,\n",
        "      dropout=dropout\n",
        "  )\n",
        "  datamodule = MedicalDataMNIST(\n",
        "      model=model_name,\n",
        "      batch_size=batch_size, # 128 batch_size is max\n",
        "      num_workers=os.cpu_count()\n",
        "  )\n",
        "\n",
        "  # Logger\n",
        "  logger = pl.loggers.TensorBoardLogger(\n",
        "      \"logs\",\n",
        "      name=\"\", \n",
        "      version=f\"trial-{trial.number}_\" + \n",
        "      f\"{model_name}_\" + \n",
        "      f\"{optimizer_name}_\" +\n",
        "      f\"bs={batch_size}_\" +\n",
        "      f\"lr={learning_rate:0.5f}\"\n",
        "  )\n",
        "\n",
        "  # Model checkpoint\n",
        "  checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    monitor=\"f1/val\",\n",
        "    dirpath=\"./best_model/\",\n",
        "    filename=f\"MedMNIST_trial-{trial.number}_{model_name}_\" + \n",
        "              \"f1-val={f1/val:.3f}\",\n",
        "    save_top_k=1,\n",
        "    mode=\"max\",\n",
        "    auto_insert_metric_name=False\n",
        "  )\n",
        "\n",
        "\n",
        "  # Trainer\n",
        "  trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        max_epochs=1,\n",
        "        gpus=torch.cuda.device_count() if torch.cuda.is_available() else None,\n",
        "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"f1/val\"),\n",
        "                   pl.callbacks.early_stopping.EarlyStopping(monitor=\"f1/val\",\n",
        "                                                             min_delta=0.001,\n",
        "                                                             patience=5),\n",
        "                   checkpoint_callback]\n",
        "  )\n",
        "\n",
        "  hyperparameters = dict(\n",
        "      model=model_name,\n",
        "      optimizer=optimizer_name,\n",
        "      lr=learning_rate,\n",
        "      betas=betas,\n",
        "      eps=eps,\n",
        "      weight_decay=weight_decay,\n",
        "      momentum=momentum,\n",
        "      alpha=alpha,\n",
        "      lambd=lambd,\n",
        "      asgd_alpha=asgd_alpha,\n",
        "      batch_size=batch_size,\n",
        "      dropout=dropout\n",
        "  )\n",
        "  trainer.logger.log_hyperparams(hyperparameters)\n",
        "  trainer.fit(model, datamodule=datamodule)\n",
        "\n",
        "  current_f1 = trainer.callback_metrics[\"f1/val\"].item()\n",
        "\n",
        "  return current_f1"
      ],
      "metadata": {
        "id": "BtcYiJwLdzRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  # Accelerators\n",
        "  print(f\"Number of CPUS: {os.cpu_count()}\")\n",
        "  print(f\"Number of GPUS: {torch.cuda.device_count() if torch.cuda.is_available() else None}\")\n",
        "\n",
        "  # Tensorboard\n",
        "  # %load_ext tensorboard\n",
        "  # %tensorboard --logdir=logs/\n",
        "\n",
        "  # Pruner\n",
        "  pruner = optuna.pruners.MedianPruner()\n",
        "\n",
        "  # Study\n",
        "  study = optuna.create_study(direction=\"maximize\", pruner=pruner, \n",
        "                              load_if_exists=True)\n",
        "  joblib.dump(study, \"study.pkl\")\n",
        "  study.optimize(objective, n_trials=100, timeout=36000) # 10h\n",
        "\n",
        "  print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
        "\n",
        "  print(\"Best trial:\")\n",
        "  trial = study.best_trial\n",
        "\n",
        "  print(\"  Value: {}\".format(trial.value))\n",
        "\n",
        "  print(\"  Params: \")\n",
        "  for key, value in trial.params.items():\n",
        "      print(\"    {}: {}\".format(key, value))\n"
      ],
      "metadata": {
        "id": "RJcdFyj8sGg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "datamodule = MedicalDataMNIST(\n",
        "      batch_size=128, # 128 batch_size is max\n",
        "      num_workers=os.cpu_count()\n",
        "  )\n",
        "\n",
        "model = MedicalMNIST.load_from_checkpoint(\n",
        "    \"/content/best_model/MedMNIST_trial-0_VGG16_f1-val=0.171.ckpt\"\n",
        ")\n",
        "\n",
        "# model = torch.nn.DataParallel(model)\n",
        "# model.to(device)\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=torch.cuda.device_count() if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "trainer.test(model)"
      ],
      "metadata": {
        "id": "MVwpx9xEwx6e"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}