{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_file.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNUZAY4UwyTEYsd//BReoE/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZAhpo9y3FvL"
      },
      "source": [
        "# Download data\n",
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d 'andrewmvd/medical-mnist'\n",
        "!unzip -q medical-mnist.zip -d data\n",
        "!rm medical-mnist.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSOXj_vP9Vj_"
      },
      "source": [
        "# Installations\n",
        "!pip install pytorch-lightning\n",
        "!pip install wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KA6ynB7HTXdR"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import Trainer\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "from pytorch_lightning.core.lightning import LightningModule\n",
        "import optuna\n",
        "from optuna.integration import PyTorchLightningPruningCallback"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables\n",
        "BEST_F1 = 0"
      ],
      "metadata": {
        "id": "gjCA5Lns1mPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P-c3p3rKjPZ"
      },
      "source": [
        "# Data preparation\n",
        "\n",
        "class MedicalDataMNIST(pl.LightningDataModule):\n",
        "    def __init__(self, num_workers=1):\n",
        "      super().__init__()\n",
        "      self.num_workers = num_workers\n",
        "      self.labels_map = {0 : \"AbdomenCT\",\n",
        "                         1 : \"BreastMRI\",\n",
        "                         2 : \"CXR\",\n",
        "                         3 : \"ChestCT\",\n",
        "                         4 : \"Hand\",\n",
        "                         5 : \"HeadCT\"}\n",
        "      self.train_transform = transforms.Compose(\n",
        "          [transforms.ColorJitter(hue=.20, saturation=.20),\n",
        "           transforms.RandomHorizontalFlip(),\n",
        "           transforms.RandomVerticalFlip(),\n",
        "           transforms.RandomRotation(10),\n",
        "           transforms.ToTensor(),\n",
        "           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])]\n",
        "      )\n",
        "      \n",
        "      self.val_test_transform = transforms.Compose(\n",
        "          [transforms.ToTensor(),\n",
        "           transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                std=[0.229, 0.224, 0.225])]\n",
        "      )\n",
        "\n",
        "    def prepare_data(self):\n",
        "      pass\n",
        "      # called only on 1 GPU\n",
        "      # ONLY DOWNLOAD!!!\n",
        "      \n",
        "    def setup(self, stage=None):\n",
        "\n",
        "      self.dataset = ImageFolder(\"./data/\")\n",
        "      train_size = int(0.7 * len(self.dataset)) # take 70% for training\n",
        "      val_size = int(0.2 * len(self.dataset)) # take 20% for validation\n",
        "      test_size = len(self.dataset) - (train_size + val_size) # take 10% for test\n",
        "      \n",
        "      self.train_set, self.val_set, self.test_set = \\\n",
        "      torch.utils.data.random_split(self.dataset, \n",
        "                                    [train_size, val_size, test_size])\n",
        "\n",
        "      self.train_set.dataset.transform = self.train_transform\n",
        "      self.val_set.dataset.transform = self.val_test_transform\n",
        "      self.test_set.dataset.transform = self.val_test_transform\n",
        "\n",
        "    def train_dataloader(self):\n",
        "      return DataLoader(self.train_set, \n",
        "                        batch_size=128, \n",
        "                        shuffle=True, \n",
        "                        num_workers=self.num_workers) # 128 batch_size is max\n",
        "\n",
        "    def val_dataloader(self):\n",
        "      return DataLoader(self.val_set, \n",
        "                        batch_size=128, \n",
        "                        num_workers=self.num_workers)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "      print(\"TEST DATALOADER\")\n",
        "      return DataLoader(self.test_set, \n",
        "                        batch_size=128, \n",
        "                        num_workers=self.num_workers)\n",
        "\n",
        "    def visualize_dataset(self):\n",
        "      # Visualizes dataset\n",
        "      figure = plt.figure(figsize=(8, 8))\n",
        "      cols, rows = 3, 3\n",
        "      for i in range(1, cols * rows + 1):\n",
        "          sample_idx = torch.randint(len(self.train_set), size=(1,)).item()\n",
        "          norm_img, label = self.train_set[sample_idx]\n",
        "          mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "          std = torch.tensor([0.229, 0.224, 0.225])\n",
        "          img = norm_img * std[:, None, None] + mean[:, None, None] \n",
        "          figure.add_subplot(rows, cols, i)\n",
        "          plt.title(self.labels_map[label])\n",
        "          plt.axis(\"off\")\n",
        "          plt.imshow(img.permute(1, 2, 0))\n",
        "      plt.show()\n",
        "\n",
        "    def visualize_dataloader(self):\n",
        "      # Display image and label\n",
        "      train_dataloader = self.train_dataloader()\n",
        "      train_features, train_labels = next(iter(train_dataloader))\n",
        "      print(f\"Feature batch shape: {train_features.size()}\")\n",
        "      print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "      norm_img = train_features[0]\n",
        "      mean = torch.tensor([0.485, 0.456, 0.406])\n",
        "      std = torch.tensor([0.229, 0.224, 0.225])\n",
        "      img = norm_img * std[:, None, None] + mean[:, None, None]\n",
        "      label = train_labels[0]\n",
        "      plt.imshow(img.permute(1, 2, 0))\n",
        "      plt.show()\n",
        "      print(f\"Label: {self.labels_map[label.item()]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZsfSpK17vhE"
      },
      "source": [
        "dm = MedicalDataMNIST()\n",
        "dm.setup()\n",
        "dm.visualize_dataset()\n",
        "dm.visualize_dataloader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sihoHTSWNoPA"
      },
      "source": [
        "# Creating model\n",
        "import torchmetrics\n",
        "\n",
        "class MedicalMNIST(LightningModule):\n",
        "    def __init__(self, model=\"EfficientNetb0\", optimaizer=\"Adam\", lr=1e-4,\n",
        "                 betas=(0.9, 0.999), eps=1e-08, weight_decay=0, momentum=0,\n",
        "                 alpha=0.99, lambd=1e-4, asgd_alpha=0.75, dropout=0.2): \n",
        "        super().__init__()\n",
        "\n",
        "        # Hyperparameters\n",
        "        # Model\n",
        "        if model == \"EfficientNetb0\":\n",
        "          # Fine tuning EfficientNetb0\n",
        "          self.name = \"EfficientNetb0\"\n",
        "          self.model = models.efficientnet_b0(pretrained=True)\n",
        "          self.model.classifier = torch.nn.Sequential(\n",
        "              torch.nn.Dropout(p=dropout, inplace=False),\n",
        "              torch.nn.Linear(in_features=self.model.classifier[1].in_features,\n",
        "                              out_features=6)\n",
        "          )\n",
        "\n",
        "        elif model == \"VGG16\":\n",
        "          # Fine tuning VGG16\n",
        "          self.name = \"VGG16\"\n",
        "          self.model = models.vgg16(pretrained=True)\n",
        "          self.model.classifier[-1] = nn.Linear(in_features=4096, \n",
        "                                                out_features=6)\n",
        "\n",
        "        elif model == \"InceptionV3\":\n",
        "          # Fine tuning InceptionV3\n",
        "          self.name = \"InceptionV3\"\n",
        "          self.model = models.inception_v3(pretrained=True)\n",
        "          # Handle the auxilary net\n",
        "          in_features = self.model.AuxLogits.fc.in_features\n",
        "          self.model.AuxLogits.fc = nn.Linear(in_features=in_features,\n",
        "                                              out_features=6)\n",
        "          # Handle the primary net\n",
        "          in_features = self.model.fc.in_features\n",
        "          self.model.fc = nn.Linear(in_features=in_features,\n",
        "                                    out_features=6)\n",
        "\n",
        "        else model == \"ResNet18\":\n",
        "          # Fine tuning ResNet18\n",
        "          self.name = \"ResNet18\"\n",
        "          self.model = models.resnet18(pretrained=True)\n",
        "          self.model.fc = nn.Linear(in_features=self.model.fc.in_features,\n",
        "                                    out_features=6)\n",
        "\n",
        "        # Optimizer\n",
        "        if optimazer == \"Adam\":\n",
        "          self.optimazer = torch.optim.Adam(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              betas=betas,\n",
        "              eps=eps,\n",
        "              weight_decay=weight_decay\n",
        "          )\n",
        "\n",
        "        elif optimazer == \"SGD\":\n",
        "          self.optimazer = torch.optim.SGD(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              momentum=momentum,\n",
        "              weight_decay=weight_decay\n",
        "          )\n",
        "          \n",
        "        elif optimaizer == \"RMSprop\":\n",
        "          self.optimazer = torch.optim.RMSprop(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              alpha=alpha,\n",
        "              eps=eps,\n",
        "              weight_decay=weight_decay,\n",
        "              momentum=momentum\n",
        "          )\n",
        "\n",
        "        else:\n",
        "          # ASGD\n",
        "          self.optimazer = torch.optim.ASGD(\n",
        "              self.parameters(),\n",
        "              lr=lr,\n",
        "              lambd=lambd,\n",
        "              alpha=asgd_alpha,\n",
        "              weight_decay=weight_decay,\n",
        "          )\n",
        "        \n",
        "        # Metrics\n",
        "        self.train_acc = torchmetrics.Accuracy()\n",
        "        self.val_acc = torchmetrics.Accuracy()\n",
        "        self.train_f1 = torchmetrics.F1(num_classes=6)\n",
        "        self.val_f1 = torchmetrics.F1(num_classes=6)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        logits = self.model(x)\n",
        "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
        "\n",
        "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
        "      \n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self.model(x)\n",
        "        val_loss = torch.nn.functional.cross_entropy(y_hat, y)\n",
        "        \n",
        "        self.log(\"val_loss\", val_loss)\n",
        "        self.log(\"val_accuaracy\", self.val_acc(y_hat, y), prog_bar=True,\n",
        "                 logger=True)\n",
        "        self.log(\"f1/val\", self.val_f1(y_hat, y), prog_bar=True,\n",
        "                 logger=True)\n",
        "        \n",
        "        return val_loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return self.optimazer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Custom callbaks\n",
        "\n",
        "# class TensorCallback(pl.Callback):\n",
        "#   \"\"\"\n",
        "#   To fill...\n",
        "#   \"\"\"\n",
        "#   def on_train_epoch_start(self, trainer, _):\n",
        "#         \"\"\" Check if we should save a checkpoint after every train epoch \"\"\"\n",
        "#         epoch = trainer.current_epoch\n",
        "#         if epoch == 1:\n",
        "#           try:\n",
        "#             from tensorboard import notebook\n",
        "\n",
        "#           except:\n",
        "\n",
        "#             # Load Tensorboard\n",
        "#             %load_ext tensorboard\n",
        "\n",
        "#             # Adjust the hight of the tensorboard\n",
        "#             from tensorboard import notebook\n",
        "#             notebook.display(height=2000)\n",
        "\n",
        "#             # Show the logs \n",
        "#             %tensorboard --logdir=lightning_logs/\n",
        "\n"
      ],
      "metadata": {
        "id": "kRUtWULV1nn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial: optuna.trial.Trial) -> float:\n",
        "  # OPTUNA objective function\n",
        "\n",
        "  # Hyperparameters\n",
        "  model_name = trial.suggest_categorical(\n",
        "      \"model_name\", [\"EfficientNetb0\", \"VGG16\", \"InceptionV3\", \"ResNet18\"]\n",
        "  )\n",
        "  optimizer_name = trial.suggest_categorical(\n",
        "      \"optimaizer_name\", [\"Adam\", \"SGD\", \"RMSprop\", \"ASGD\"]\n",
        "  )\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32, 64, 128])\n",
        "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1)\n",
        "  betas = (trial.suggest_uniform(\"beta_1\", 0.8, 0.95),\n",
        "            trial.suggest_uniform(\"beta_2\", 0.995, 0.9999))\n",
        "  eps = trial.suggest_loguniform(\"eps\", 1e-09, 1e-07)\n",
        "  weight_decay = trial.suggest_float(\"weight_decay\", 1e-10, 1e-3, log=True)\n",
        "  momentum = trial.suggest_float(\"momentum\", 1e-5, 1e-1, log=True)\n",
        "  alpha = trial.suggest_uniform(\"alpha\", 0.9, 1)\n",
        "  lambd = trial.suggest_float(\"lambd\", 1e-5, 1e-2, log=True)\n",
        "  asgd_alpha = trial.suggest_uniform(\"asgd_alpha\", 0.7, 0.8)\n",
        "  dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.2, 0.3, 0.4, 0.5])\n",
        "\n",
        "  # Model and data\n",
        "  model = MedicalMNIST(\n",
        "      model=model_name,\n",
        "      optimizer=optimizer_name,\n",
        "      lr=learning_rate,\n",
        "      betas=betas,\n",
        "      eps=eps,\n",
        "      weight_decay=weight_decay,\n",
        "      momentum=momentum,\n",
        "      alpha=alpha,\n",
        "      lambd=lambd,\n",
        "      asgd_alpha=asgd_alpha,\n",
        "      dropout=dropout\n",
        "  )\n",
        "  datamodule = MedicalDataMNIST(\n",
        "      batch_size=batch_size,\n",
        "      num_workers=os.cpu_count()\n",
        "  )\n",
        "\n",
        "  # Logger\n",
        "  logger = pl.loggers.TensorBoardLogger(\n",
        "      \"logs\", \n",
        "      name=None,\n",
        "      version=f\"trial_{trial.number}_{model_name}_{learning_rate}\"\n",
        "  )\n",
        "\n",
        "  # Trainer\n",
        "  trainer = pl.Trainer(\n",
        "        logger=logger,\n",
        "        checkpoint_callback=False,\n",
        "        max_epochs=20,\n",
        "        gpus=torch.cuda.device_count() if torch.cuda.is_available() else None,\n",
        "        callbacks=[PyTorchLightningPruningCallback(trial, monitor=\"f1/val\")],\n",
        "  )\n",
        "\n",
        "  hyperparameters = dict(\n",
        "      model=model_name,\n",
        "      optimizer=optimizer_name,\n",
        "      lr=learning_rate,\n",
        "      betas=betas,\n",
        "      eps=eps,\n",
        "      weight_decay=weight_decay,\n",
        "      momentum=momentum,\n",
        "      alpha=alpha,\n",
        "      lambd=lambd,\n",
        "      asgd_alpha=asgd_alpha,\n",
        "      batch_size=batch_size,\n",
        "      dropout=dropout\n",
        "  )\n",
        "  trainer.logger.log_hyperparams(hyperparameters)\n",
        "  trainer.fit(model, datamodule=datamodule)\n",
        "\n",
        "  # Save model\n",
        "  current_f1 = trainer.callback_metrics[\"f1/val\"].item()\n",
        "  if trial.number == 0:\n",
        "    BEST_F1 = current_f1\n",
        "    dir = \"./best_model/\"\n",
        "    file_name = f\"trial_{trial.number}_{model_name}_f1={current_f1}.ckpt\"\n",
        "    ckpt_path = os.path.join(dir, file_name)\n",
        "    trainer.save_checkpoint(ckpt_path)\n",
        "  elif current_f1 > BEST_F1:\n",
        "    BEST_F1 = current_f1\n",
        "    dir = \"./best_model/\"\n",
        "    files = glob.glob(dir + '*')\n",
        "    for f in files:\n",
        "      os.remove(f)\n",
        "    file_name = f\"trial_{trial.number}_{model_name}_f1={current_f1}.ckpt\"\n",
        "    ckpt_path = os.path.join(dir, file_name)\n",
        "    trainer.save_checkpoint(ckpt_path)\n",
        "\n",
        "  return current_f1"
      ],
      "metadata": {
        "id": "BtcYiJwLdzRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN\n",
        "# %load_ext tensorboard\n",
        "%tensorboard --logdir=logs/\n",
        "# Import wandb\n",
        "# from pytorch_lightning.loggers import WandbLogger\n",
        "# from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "# wandb_logger = WandbLogger(project='Medical MNIST', log_model='all')\n",
        "\n",
        "# Number of cpus and gpus\n",
        "NUM_DATALOADER_WORKERS = os.cpu_count()\n",
        "print(f\"Number of CPUS: {NUM_DATALOADER_WORKERS}\")\n",
        "NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else None\n",
        "print(f\"Number of GPUS: {NUM_GPUS}\")\n",
        "\n",
        "medical_mnist_data = MedicalDataMNIST(num_workers=NUM_DATALOADER_WORKERS)\n",
        "model = MedicalMNIST()\n",
        "\n",
        "# saves a file like: my/path/sample-mnist-epoch=02-val_loss=0.32.ckpt\n",
        "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
        "    dirpath=\"./best_models/\",\n",
        "    monitor=\"val_f1\",\n",
        "    filename=\"medicalMNIST\" + f\"-{model.name}-\" + \"{epoch:02d}-{val_f1:.2f}\",\n",
        "    mode=\"max\",\n",
        ")\n",
        "logger = pl.loggers.TensorBoardLogger(\"logs\", name=model.name)\n",
        "# Train\n",
        "trainer = Trainer(gpus=NUM_GPUS, \n",
        "                  max_epochs=4, \n",
        "                  logger=logger,\n",
        "                  callbacks=[checkpoint_callback])\n",
        "\n",
        "# wandb_logger.watch(model)\n",
        "trainer.fit(model, datamodule=medical_mnist_data)"
      ],
      "metadata": {
        "id": "RJcdFyj8sGg1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}